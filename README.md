# ğŸ§¬ Wastewater Haplotype Inference
**Bayesian haplotype inference using short-read viral sequencing data from wastewater samples**

---

## ğŸ“– Overview

This project implements a Bayesian framework to infer viral haplotype frequencies from deep-sequenced SARS-CoV-2 wastewater data. We first run two workflows to precompute the fixed inputs for inference: (1) read based SNV and encoding pipeline and (2) consensus panel prior pipeline. Then a collapsed Gibbs sampler is used to estimate the haplotype frequencies.

---

## ğŸ§  Model Description

### Data and notation

* A Deep sequenced wastewater sample with $J$ biallelic sites is indexed by $j=1,\dots,J$.
* Each site has REF/ALT alleles; a haplotype $h \in \lbrace 1,\ldots,H \rbrace$ is a length $J$ allele vector; with biallelic sites, $H=2^J$.
* Short reads are grouped into **read patterns** $r = 1, 2, \ldots, R$; each pattern is a length $J$ string over `{0,1,-}`(`0` = REF, `1` = ALT,  = not covered).
* Let $k_r$ be the number of reads with pattern $r$; $\sum_{r=1}^R k_r = N$. Write $\mathbf{k}=(k_1,\dots,k_R)^\top$.

---

### Compatibility and pattern probabilities

Let $C \in \lbrace 0,1 \rbrace^{R\times H}$ be the compatibility matrix: 

$$
C_{rh} =
\begin{cases}
1, & \text{if pattern } r \text{ is compatible with haplotype } h, \\
0, & \text{otherwise.}
\end{cases}
$$

Define

$$
A_{rh}=\frac{C_{rh}}{Z_h},\qquad
Z_h=\sum_{r=1}^R C_{rh},\qquad
\sum_{r=1}^R A_{rh}=1
$$

for each $h$.

Let $\mathbf p=(p_1,\dots,p_H)^\top$ denote haplotype frequencies on the $H$-simplex.

The pattern distribution is

$$
\mathbf q = A \mathbf p
$$

$$
q_r = \sum_{h=1}^H A_{rh} p_h
$$

---

### Read-pattern likelihood

Given $\mathbf p$, the pattern counts follow a multinomial model:

$$
\mathbf K|\mathbf p \sim \mathrm{Multinomial}(N;\mathbf q),
$$

and the log-likelihood (without constants) is:

$$
\ell(\mathbf p;\mathbf k) = \sum_{r=1}^R k_r \log q_r = \mathbf k^\top \log(A\mathbf p).
$$

---

### Dirichlet prior on haplotype frequencies

We place a Dirichlet prior on $\mathbf p$:

$$
\mathbf p \sim \mathrm{Dirichlet}(\boldsymbol\alpha),
\quad
\boldsymbol\alpha = \alpha_0\\mathbf w.
$$

- **Informative prior:** $\mathbf w$ is derived from normalized GISAID haplotype frequencies.  
  To avoid zeros, a small floor $\varepsilon$ is applied before renormalization. 

- **Non-informative prior:** $\mathbf w = \tfrac{1}{H}\mathbf 1$ and $\alpha_0 = H$.

---

### Collapsed Gibbs sampling via data augmentation

We introduce allocation counts $y_{rh}$: number of reads with pattern $r$ that are attributed to haplotype $h$.  
They satisfy $\sum_h y_{rh}=k_r$ for each r.
Our model alternates between these two steps:

1. **Allocation step:**
   
$$
(y_{r1},\dots,y_{rH})|\mathbf p
\sim \mathrm{Multinomial}\\big(k_r;\ w_{r1},\dots,w_{rH}\big),
\quad
w_{rh} = \frac{A_{rh}p_h}{q_r}.
$$

3. **Update step:**
   
$$
\mathbf p|\mathbf y \sim 
\mathrm{Dirichlet}(\alpha_1+n_1,\dots,\alpha_H+n_H),
\quad n_h = \sum_r y_{rh}.
$$

This results in a **collapsed Gibbs sampler** with Dirichletâ€“multinomial conjugacy.

---

### Initialization, constraints, reproducibility

There are two parallel chains:
- **Data-based initialization:** product over sites: $\hat f_j$ for ALT allele frequency, $1-\hat f_j$ for REF; 
- **Uniform initialization:** $\mathbf p^{(0)} = \tfrac{1}{H}\mathbf 1$.  

At each iteration, we set:

$$
p_h \leftarrow \max(p_h,\varepsilon), \quad 
\text{renormalize s.t. } \sum_h p_h=1.
$$

The typical configurations are:
$n_{\text{iter}}=10^5$, burn-in $B=2\times10^4$, no thinning, fixed seeds.

---

### Posterior diagnostics

We compute:
- Log-posterior traces
- $\hat R$ convergence diagnostic
- Posterior mean $\pm$ 95% credible intervals for top-\(K\) haplotypes
- ALT allele posterior traces
  
  $$
  f_j^{(t)} = \sum_h a_{hj} p_h^{(t)}
  $$

---

## âš™ï¸ Pipeline structure
â”œâ”€â”€ read_based_pipeline.sh         # Read-based SNV and Encoding pipeline
â”œâ”€â”€ consensus_based_pipeline.sh    # Consensus Panel Prior pipeline
â”œâ”€â”€ get_snvs.py                    # SNV extraction through LoFreq
â”œâ”€â”€ filter_lofreq_vcf.py           # Variant filtering (VAF, RAF, DP)
â”œâ”€â”€ extract_snv_table.py           # Generate SNVs summary table
â”œâ”€â”€ read_encoding.py               # Get read patterns {0,1,-}
â”œâ”€â”€ compute_haplotype_freqs_allele.py       # Compute data-based initial haplotype frequencies
â”œâ”€â”€ consensus_summarize_variant_alleles.py  # Summarize consensus allele frequencies
â”œâ”€â”€ consensus_haplotype_freq.py    # Compute panel-based haplotype frequencies as priors
â”œâ”€â”€ gibbs_sampler.py               # Whole collapsed Gibbs sampler code for haplotype inference

---

## ğŸ§© Workflow

![Pipeline diagram](images/pipeline_workflow.png)

**Step 1.** Reads alignment â†’ call SNVs â†’ summarize read patterns  
**Step 2.** Consensus genomes alignment â†’ compute panel imformative prior  
**Step 3.** Run Gibbs sampler to infer posterior haplotype frequencies  
**Step 4.** Summarize diagnostics and credible intervals  

---

## ğŸš€ Run the pipeline

```bash
# Step 1. Run read-based pipeline

Paired-end (recommended)
./read_based_pipeline.sh \
  -r reference.fasta \
  -1 short_read_sequencing_R1.fastq \
  -2 short_read_sequencing_R2.fastq \
  --primer-bed primers.bed \
  -t 8 \
  --min-cov 30 \
  --min-alt-freq 0.10 \
  --min-ref-freq 0.10 \
  --outdir results/read_based_output

Single-file interleaved
./read_based_pipeline.sh \
  -r reference.fasta \
  -q short_read_sequencing.fastq \
  --interleaved \
  --primer-bed primers.bed \
  -t 8 \
  --outdir results/read_based_output

Single-end
./read_based_pipeline.sh \
  -r reference.fasta \
  -q short_read_sequencing.fastq \
  -t 8 \
  --outdir results/read_based_output

# Step 2. Run consensus-panel pipeline
chmod +x consensus_based_pipeline.sh
./consensus_based_pipeline.sh \
  -r reference.fasta \
  -s samples.fasta \
  --readdir path/to/read_based_output \
  [-t threads] \
  [-o outdir]

# Step 3. Perform Gibbs sampling
```

---

## ğŸ“Š Dependencies
	â€¢	Python â‰¥ 3.11
	â€¢	NumPy, SciPy, Pandas, Matplotlib
	â€¢	BWA, MAFFT, LoFreq
	â€¢	Bash â‰¥ 4.0
	
---

## ğŸ§‘â€ğŸ’» Implementation details

All computations are vectorized with NumPy and sparse matrix operations in SciPy.
The compatibility matrix (C) is stored in CSR format, and the log-likelihood term is efficiently computed via sparseâ€“dense multiplication.
	
---

## ğŸ“¬ Contact

For questions or collaboration:
Menghui (Mona) Chen
ğŸ“§ menghui.chen@emory.edu


